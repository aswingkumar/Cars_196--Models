{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1120177,"sourceType":"datasetVersion","datasetId":629073},{"sourceId":41446,"sourceType":"modelInstanceVersion","modelInstanceId":34855,"modelId":49029}],"dockerImageVersionId":30018,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"###### Computer vision: Classifying car make, model and year\n\nComputer vision could potentially be used to automate traffic censuses and other tasks that require identification of vehicles.\nThe <a href=\"https://www.tensorflow.org/datasets/catalog/cars196\">cars196</a> dataset contains 16,185 images of 196 different types of cars, which\ncan be used to train a supervised learning system to determine the make and model of a vehicle in a photograph.","metadata":{}},{"cell_type":"markdown","source":"# Prelude\nStanford cars are the collection of images of cars that are from the dataset \"Cars196\". As the dataset from \"Cars196\" included in Tensorflow has been rendered disabled since the original author of the dataset has removed the original link to the dataset. Now, this dataset has been the only available dataset apart from another \"Stanford Cars\" dataset on Kaggle. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_datasets as tfds\nfrom matplotlib import pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-29T20:13:44.296091Z","iopub.execute_input":"2024-04-29T20:13:44.296388Z","iopub.status.idle":"2024-04-29T20:13:49.733452Z","shell.execute_reply.started":"2024-04-29T20:13:44.29636Z","shell.execute_reply":"2024-04-29T20:13:49.732747Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"First, let's get the data. This dataset only works on Kaggle as setting download to True will only trigger errors from the Tensorflow framework.","metadata":{}},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/cars196'\n\n[train_ds, test_ds], ds_info = tfds.load(\n    \"cars196\",\n    split=[\"train\", \"test\"],\n    as_supervised=True,  # Include labels\n    with_info=True,\n    download=False,\n    data_dir=DATA_DIR,\n)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2024-04-29T20:14:40.302934Z","iopub.execute_input":"2024-04-29T20:14:40.303302Z","iopub.status.idle":"2024-04-29T20:14:42.268491Z","shell.execute_reply.started":"2024-04-29T20:14:40.30327Z","shell.execute_reply":"2024-04-29T20:14:42.267376Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, let's use the built-in visualization function to show some example images:","metadata":{}},{"cell_type":"code","source":"tfds.visualization.show_examples(train_ds, ds_info)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T11:30:26.604506Z","iopub.execute_input":"2024-04-29T11:30:26.604884Z","iopub.status.idle":"2024-04-29T11:30:28.443667Z","shell.execute_reply.started":"2024-04-29T11:30:26.604851Z","shell.execute_reply":"2024-04-29T11:30:28.442552Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Standardizing the data\nOur raw images have a variety of sizes. In addition, each pixel consists of 3 integer values between 0 and 255 (RGB level values). This isn't a great fit for feeding a neural network. We need to do 2 things:\n\n* Standardize to a fixed image size.\n* Normalize pixel values between -1 and 1. We'll do this using a Normalization layer as part of the model itself.\n\nIn general, it's a good practice to develop models that take raw data as input, as opposed to models that take already-preprocessed data. The reason being that, if your model expects preprocessed data, any time you export your model to use it elsewhere (in a web browser, in a mobile app), you'll need to reimplement the exact same preprocessing pipeline. This gets very tricky very quickly. So we should do the least possible amount of preprocessing before hitting the model.\n\nHere, we'll do image resizing in the data pipeline (because a deep neural network can only process contiguous batches of data), and we'll do the input value scaling as part of the model, when we create it.","metadata":{}},{"cell_type":"code","source":"height, width = 224, 224\n\ntrain_ds = train_ds.map(lambda x, y: (tf.image.resize(x,(height,width)), y)) \n#remember hat y is the dependent variable -label\n# in this case, y is the category of the car\n\ntest_ds = test_ds.map(lambda x, y: (tf.image.resize(x,(height,width)), y)) ","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:14:46.277089Z","iopub.execute_input":"2024-04-29T20:14:46.277431Z","iopub.status.idle":"2024-04-29T20:14:46.35333Z","shell.execute_reply.started":"2024-04-29T20:14:46.277401Z","shell.execute_reply":"2024-04-29T20:14:46.352542Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocessing: Resizing and random data augmentation\n\nWhen you don't have a large image dataset, it's a good practice to artificially introduce sample diversity by applying random yet realistic transformations to the training images, such as random horizontal flipping or small random rotations. This helps expose the model to different aspects of the training data while slowing down overfitting.\n\nAdditionally, let's the data and use caching and prefetching to optimize load speed:","metadata":{}},{"cell_type":"code","source":"batch_size = 32 #to optimize the load speed\n\ndef augment_func(image,label):\n  image = tf.image.resize_with_crop_or_pad(image, height + 6, width +6) \n#randomizing the size of the car, so the model doesnt identify cars by size\n  image = tf.image.random_crop(image, size=[height, width,3])\n#a border in the image, so kind of reducing the size\n  image = tf.image.random_flip_left_right(image)\n  image = tf.image.random_hue(image, 0.2) #randomizing the color (red, blue, green...)\n# identifying the car even if it is different color\n  image = tf.image.random_contrast(image, 0.5, 2) \n#parameters that he obtained from tutorials\n  image = tf.image.random_saturation(image, 0, 2)\n  return image, label\n\n# RANDOMIZING THE CHARACTERISTICS THAT ARE NOT IMPORTANT\n# a human can identify a car, no matter the color and size in the image. However, machine learning algorithm have struggled on this\n\n\ntrain_ds = train_ds.cache().map(augment_func).shuffle(100).batch(batch_size).prefetch(buffer_size=10)\ntest_ds = test_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n    #.cache and prefetch are to make it faster\n    # shuffling so the images are not in the same order every time. 32 is the standard batch size","metadata":{"execution":{"iopub.status.busy":"2024-04-29T12:04:23.559353Z","iopub.execute_input":"2024-04-29T12:04:23.559706Z","iopub.status.idle":"2024-04-29T12:04:23.722429Z","shell.execute_reply.started":"2024-04-29T12:04:23.559671Z","shell.execute_reply":"2024-04-29T12:04:23.721645Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's visualize what the first 18 images of the first batch looks like after various random transformations.\n\nNote that because the augmentations in the previous cell are applied randomly, these images will look different everytime they are run through the model during training.","metadata":{}},{"cell_type":"code","source":"for (image_batch, label_batch) in train_ds.take(18):\n    print(label_batch)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T09:58:17.922923Z","iopub.status.idle":"2024-04-29T09:58:17.923384Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i,(image_batch, label_batch) in enumerate(train_ds.take(18)):\n    print(i,label_batch)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T09:58:17.924346Z","iopub.status.idle":"2024-04-29T09:58:17.924808Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,20))\n\nfor i,(image_batch,label) in enumerate(train_ds.take(18)):\n   #the item + index number if we use enumerate(). Also, we are taking the first 18 batches \n    ax=plt.subplot(6, 3, i + 1) #index is 1 for the first\n    plt.imshow(image_batch[0].numpy().astype(\"int32\"))\n   # we are taking the first image, converting to numpy as it was a tensor and transforming it to an integer     \n    plt.title(ds_info.features[\"label\"].names[int(label[0])])\n    plt.axis(\"off\")\n\n# printing the first, 18 images","metadata":{"execution":{"iopub.status.busy":"2024-04-29T09:58:17.925805Z","iopub.status.idle":"2024-04-29T09:58:17.926259Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build a model\n\nNow let's built a model.\n\n1. We add a Normalization layer to scale input values (initially in the [0, 255] range) to the [-1, 1] range, because this is the format that is expected by the pre-trained model that comes next.\n1. We start with a pre-trained model that's trained on the [ImageNet](http://image-net.org/about-overview) dataset, which includes a large number of images with a large number of different labels, but doesn't not include as much specificity regarding vehicle types as the cars196 dataset does. Training these models from scratch is tricky; it is much easier to start with a pre-trained model and fine tune it for use for a different task.\n3. We add our own classification layer at the end of the model, with 96 outputs representing our 96 vehicle classes, and \"softmax\" activation which forces the output values to all be between 0 and 1, and to all sum to 1.\n4. We add a Dropout layer before the above classification layer, for regularization.\n\n\nWe need the number of outputs in the final layer to equal the number of variables or classes we want to predict: in this case, 196 vehicle types. \nWe use a softmax activation on the on the final layer for classification problems, but if we want to use this model for regression we would only have to change the number of desired outputs and set `activation=None`.","metadata":{}},{"cell_type":"markdown","source":"Here, we do imports for the necessary library for the first and possibly other CNN architecture models.","metadata":{}},{"cell_type":"code","source":"from keras.applications.resnet50 import ResNet50\nfrom keras.applications import VGG16\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:14:55.248811Z","iopub.execute_input":"2024-04-29T20:14:55.249197Z","iopub.status.idle":"2024-04-29T20:14:55.293977Z","shell.execute_reply.started":"2024-04-29T20:14:55.249163Z","shell.execute_reply":"2024-04-29T20:14:55.293307Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Model Checkpoint provides a fast saving method by recording the result of each epochs, and selecting from all the epochs to a best model. That will be saved to the Kaggle directory.","metadata":{}},{"cell_type":"code","source":"checkpoint_filepath = '/kaggle/working/'\nmodel_checkpoint_callback = ModelCheckpoint(\n        filepath=checkpoint_filepath,\n        monitor='val_accuracy',\n        mode='max',\n        save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T11:30:49.502875Z","iopub.execute_input":"2024-04-29T11:30:49.503282Z","iopub.status.idle":"2024-04-29T11:30:49.508089Z","shell.execute_reply.started":"2024-04-29T11:30:49.503244Z","shell.execute_reply":"2024-04-29T11:30:49.507067Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fine-tune the model\n\nWe use a relatively low learning rate to prevent the model from unlearning what it learned when being trained on the larger imagenet dataset.","metadata":{}},{"cell_type":"markdown","source":"Finally, we can save the model for later use. If you are doing this on Kaggle, there is an option to download the saved file in the panel on the right side of the screen. It is advisable that to use gpu as accelerator as the running speed of the cpu on training model will be exceptionally slow.","metadata":{}},{"cell_type":"markdown","source":"# First Model: ResNet50","metadata":{}},{"cell_type":"code","source":"base_model = ResNet50(\n    weights=\"imagenet\",\n    input_shape=(height, width, 3),\n    include_top=False, \n)  \n\n#Freeze\nbase_model.trainable = True\n\n#Declare input layer\ninputs = tf.keras.Input(shape=(height, width, 3))\n\n#Normalization Layer\nnorm_layer = keras.layers.experimental.preprocessing.Normalization()\nmean = np.array([127.5] * 3)\nvar = mean ** 2\n# Scale inputs to [-1, +1]\nx = norm_layer(inputs)\nnorm_layer.set_weights([mean, var])\n\n#ResNet50 Architecture\nx = base_model(x, training=False) \nx = keras.layers.GlobalAveragePooling2D()(x) \n\n#Dropout to improve result and reduce overfitting\nx = keras.layers.Dropout(0.5)(x)  \nx = keras.layers.Dense(512, activation='relu')(x)\nnum_outputs = ds_info.features['label'].num_classes \noutputs = keras.layers.Dense(num_outputs, activation=\"softmax\")(x) \n\n#Here we don't use Sequential as it provides a worse result.\nmodel = keras.Model(inputs, outputs)\n\n#Summary of the model layers\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-29T09:58:17.931534Z","iopub.status.idle":"2024-04-29T09:58:17.932006Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Fine Tuning\n* Training on 50 epochs\n* Lower learning rate so model will not forget what it learnt\n* SparseCategorical as there are too many classes in our data","metadata":{}},{"cell_type":"code","source":"learning_rate = 1.0e-5 #low learning rate so the model does not forget \n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(), #for cases with multiple categories for conversions as shown below\n    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n\n)\n\nepochs = 50\nhistory = model.fit(train_ds, epochs=epochs, validation_data=test_ds)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T09:58:17.932932Z","iopub.status.idle":"2024-04-29T09:58:17.933562Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Plotting the accuracy graph\nplt.plot(history.history['val_sparse_categorical_accuracy'])\nplt.title('model validate accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-29T09:58:17.936096Z","iopub.status.idle":"2024-04-29T09:58:17.936541Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Callback for EarlyStopping when the model has stopped improving\ncallback = keras.callbacks.EarlyStopping(monitor='loss', patience =5)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T09:59:43.490446Z","iopub.execute_input":"2024-04-29T09:59:43.490826Z","iopub.status.idle":"2024-04-29T09:59:43.495116Z","shell.execute_reply.started":"2024-04-29T09:59:43.49079Z","shell.execute_reply":"2024-04-29T09:59:43.49424Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#This run is just to test the saved model\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1.0e-5),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(), #for cases with multiple categories for conversions as shown below\n    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n\n)\n\nhistory = model.fit(train_ds, epochs=5, callbacks=[callback], validation_data=test_ds)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T10:00:05.471042Z","iopub.execute_input":"2024-04-29T10:00:05.471415Z","iopub.status.idle":"2024-04-29T10:05:22.961793Z","shell.execute_reply.started":"2024-04-29T10:00:05.471378Z","shell.execute_reply":"2024-04-29T10:05:22.961028Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Saving the model into a .h5 file\nmodel.save(\"/kaggle/working/model_resnet_final.h5\", save_format=\"h5\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T09:58:17.941476Z","iopub.status.idle":"2024-04-29T09:58:17.94211Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['val_sparse_categorical_accuracy'])\nplt.title('model validate accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-29T10:05:34.010565Z","iopub.execute_input":"2024-04-29T10:05:34.010922Z","iopub.status.idle":"2024-04-29T10:05:34.176948Z","shell.execute_reply.started":"2024-04-29T10:05:34.010893Z","shell.execute_reply":"2024-04-29T10:05:34.17615Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['sparse_categorical_accuracy'])\nplt.title('model training accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-29T10:05:36.649636Z","iopub.execute_input":"2024-04-29T10:05:36.649982Z","iopub.status.idle":"2024-04-29T10:05:36.81307Z","shell.execute_reply.started":"2024-04-29T10:05:36.649953Z","shell.execute_reply":"2024-04-29T10:05:36.812286Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#This helps us to load the saved model\nmodel = load_model('/kaggle/input/resnetv1/tensorflow2/v4.1/3/model_resnet_final.h5')","metadata":{"execution":{"iopub.status.busy":"2024-04-29T10:36:09.637641Z","iopub.execute_input":"2024-04-29T10:36:09.637991Z","iopub.status.idle":"2024-04-29T10:36:16.327355Z","shell.execute_reply.started":"2024-04-29T10:36:09.637962Z","shell.execute_reply":"2024-04-29T10:36:16.326609Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Importing more library to calculate and evaluate the score of the model\nfrom sklearn.metrics import f1_score, precision_score, recall_score","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:15:05.569069Z","iopub.execute_input":"2024-04-29T20:15:05.569407Z","iopub.status.idle":"2024-04-29T20:15:06.113127Z","shell.execute_reply.started":"2024-04-29T20:15:05.569371Z","shell.execute_reply":"2024-04-29T20:15:06.112384Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Create two array of empty for labels and images\nlabels = []\nimages = []\n\n#Limit the size of the figure plotted\nplt.figure(figsize=(10,20))\n\n#From the test dataset, take the first ten and plot them out with their respective labels\nfor i,(image_batch,label) in enumerate(test_ds.take(10)):\n    ax=plt.subplot(6, 3, i + 1)\n    plt.imshow(image_batch.numpy().astype(\"int32\"))\n    plt.title(ds_info.features['label'].str2int(ds_info.features[\"label\"].names[int(label)]))\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T11:12:51.289347Z","iopub.execute_input":"2024-04-29T11:12:51.289746Z","iopub.status.idle":"2024-04-29T11:12:51.96399Z","shell.execute_reply.started":"2024-04-29T11:12:51.289713Z","shell.execute_reply":"2024-04-29T11:12:51.963085Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Using the same method for plotting the test dataset first 10 elements, using it to let the model predict\nimages = []\nlabels = []\n\n#take(z) will take elements from 0 to z, however, prediction can be only done on one image\n#skip(x).take(1) will be done to take skip x number of element and take the next one\nfor i,(image_batch,label) in enumerate(test_ds.take(1)):\n    image_batch = image_batch.numpy()\n    label = label.numpy()\n    image_batch = np.expand_dims(image_batch, axis=0)\n    print(image_batch.shape)\n    images.append(image_batch)\n    labels.append(label)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T13:42:53.12653Z","iopub.execute_input":"2024-04-29T13:42:53.126853Z","iopub.status.idle":"2024-04-29T13:42:53.179035Z","shell.execute_reply.started":"2024-04-29T13:42:53.126826Z","shell.execute_reply":"2024-04-29T13:42:53.178289Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Use the model to predict the image\nclasses = model.predict(images)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Print out the predicted class\nclasses = np.argmax(classes)\nprint(classes)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T11:24:35.959171Z","iopub.execute_input":"2024-04-29T11:24:35.959529Z","iopub.status.idle":"2024-04-29T11:24:35.964273Z","shell.execute_reply.started":"2024-04-29T11:24:35.959478Z","shell.execute_reply":"2024-04-29T11:24:35.963414Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Prediction array based on the true label.\ntrue_label = [141, 2, 87, 35, 189, 111, 154, 78, 98, 94]\npredicted_label = []","metadata":{"execution":{"iopub.status.busy":"2024-04-29T11:24:41.224829Z","iopub.execute_input":"2024-04-29T11:24:41.225159Z","iopub.status.idle":"2024-04-29T11:24:41.230037Z","shell.execute_reply.started":"2024-04-29T11:24:41.225132Z","shell.execute_reply":"2024-04-29T11:24:41.228912Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Calculating preicison, f1 and recall\nprint(precision_score(true_label, predicted_label, average = 'weighted'))\nprint(f1_score(true_label, predicted_label, average = 'weighted'))\nprint(recall_score(true_label, predicted_label, average = 'weighted'))","metadata":{"execution":{"iopub.status.busy":"2024-04-29T11:28:41.341347Z","iopub.execute_input":"2024-04-29T11:28:41.341709Z","iopub.status.idle":"2024-04-29T11:28:41.352506Z","shell.execute_reply.started":"2024-04-29T11:28:41.34168Z","shell.execute_reply":"2024-04-29T11:28:41.351682Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# VGG16","metadata":{}},{"cell_type":"code","source":"#Pretrained VGG16 base model\nbase_model = VGG16(\n    weights=\"imagenet\",\n    input_shape=(224, 224, 3),\n    include_top=False, \n)  \n\n#Freeze\nbase_model.trainable = False\n\n#The VGG16 architecture\nmodel = Sequential()\nmodel.add(base_model)\nmodel.add(Flatten())\nmodel.add(Dense(4096, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(4096, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1000, activation='softmax'))\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-29T12:15:01.103564Z","iopub.execute_input":"2024-04-29T12:15:01.103905Z","iopub.status.idle":"2024-04-29T12:15:01.486844Z","shell.execute_reply.started":"2024-04-29T12:15:01.103877Z","shell.execute_reply":"2024-04-29T12:15:01.48609Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Fine Tuning","metadata":{}},{"cell_type":"code","source":"learning_rate = 1.0e-5 #low learning rate so the model does not forget \n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(), #for cases with multiple categories for conversions as shown below\n    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n)\n\nepochs = 50\nhistory = model.fit(train_ds, epochs=epochs, validation_data=test_ds)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T12:15:09.773398Z","iopub.execute_input":"2024-04-29T12:15:09.773722Z","iopub.status.idle":"2024-04-29T12:44:10.263349Z","shell.execute_reply.started":"2024-04-29T12:15:09.773694Z","shell.execute_reply":"2024-04-29T12:44:10.262652Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['val_sparse_categorical_accuracy'])\nplt.title('model training accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-29T12:45:53.079634Z","iopub.execute_input":"2024-04-29T12:45:53.07998Z","iopub.status.idle":"2024-04-29T12:45:53.226941Z","shell.execute_reply.started":"2024-04-29T12:45:53.079949Z","shell.execute_reply":"2024-04-29T12:45:53.22619Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"/kaggle/working/model_vgg_v1.h5\", save_format=\"h5\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T12:46:34.734643Z","iopub.execute_input":"2024-04-29T12:46:34.734972Z","iopub.status.idle":"2024-04-29T12:46:38.519259Z","shell.execute_reply.started":"2024-04-29T12:46:34.734944Z","shell.execute_reply":"2024-04-29T12:46:38.518304Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = load_model('/kaggle/working/model_vgg_v1.h5')\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1.0e-5),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(), #for cases with multiple categories for conversions as shown below\n    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n\n)\n\nhistory = model.fit(train_ds, epochs=50, validation_data=test_ds)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T12:47:53.417152Z","iopub.execute_input":"2024-04-29T12:47:53.417591Z","iopub.status.idle":"2024-04-29T13:16:53.457704Z","shell.execute_reply.started":"2024-04-29T12:47:53.417557Z","shell.execute_reply":"2024-04-29T13:16:53.45694Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['sparse_categorical_accuracy'])\nplt.title('model training accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-29T13:35:06.901541Z","iopub.execute_input":"2024-04-29T13:35:06.901875Z","iopub.status.idle":"2024-04-29T13:35:07.081904Z","shell.execute_reply.started":"2024-04-29T13:35:06.901845Z","shell.execute_reply":"2024-04-29T13:35:07.080776Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"/kaggle/working/model_vgg_v2.h5\", save_format=\"h5\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T13:40:32.750233Z","iopub.execute_input":"2024-04-29T13:40:32.750596Z","iopub.status.idle":"2024-04-29T13:40:36.923022Z","shell.execute_reply.started":"2024-04-29T13:40:32.750562Z","shell.execute_reply":"2024-04-29T13:40:36.922163Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = load_model('/kaggle/input/vggnet/tensorflow2/vggv2/1/model_vgg_v2.h5')","metadata":{"execution":{"iopub.status.busy":"2024-04-29T14:02:41.441093Z","iopub.execute_input":"2024-04-29T14:02:41.441445Z","iopub.status.idle":"2024-04-29T14:02:56.964934Z","shell.execute_reply.started":"2024-04-29T14:02:41.441409Z","shell.execute_reply":"2024-04-29T14:02:56.963981Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images = []\nlabels = []\n\nfor i,(image_batch,label) in enumerate(test_ds.skip(9).take(1)):\n    image_batch = image_batch.numpy()\n    label = label.numpy()\n    image_batch = np.expand_dims(image_batch, axis=0)\n    images.append(image_batch)\n    labels.append(label)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:30:39.341667Z","iopub.execute_input":"2024-04-29T20:30:39.342035Z","iopub.status.idle":"2024-04-29T20:30:39.457698Z","shell.execute_reply.started":"2024-04-29T20:30:39.341999Z","shell.execute_reply":"2024-04-29T20:30:39.456942Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(images)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:25:04.802914Z","iopub.execute_input":"2024-04-29T20:25:04.803285Z","iopub.status.idle":"2024-04-29T20:25:04.809975Z","shell.execute_reply.started":"2024-04-29T20:25:04.803253Z","shell.execute_reply":"2024-04-29T20:25:04.809191Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classes = model.predict(images)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:30:41.116074Z","iopub.execute_input":"2024-04-29T20:30:41.116414Z","iopub.status.idle":"2024-04-29T20:30:41.156858Z","shell.execute_reply.started":"2024-04-29T20:30:41.116386Z","shell.execute_reply":"2024-04-29T20:30:41.156068Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classes = np.argmax(classes)\nprint(classes)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:30:42.420961Z","iopub.execute_input":"2024-04-29T20:30:42.421313Z","iopub.status.idle":"2024-04-29T20:30:42.426222Z","shell.execute_reply.started":"2024-04-29T20:30:42.421284Z","shell.execute_reply":"2024-04-29T20:30:42.425206Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"true_label = [141, 2, 87, 35, 189, 111, 154, 78, 98, 94]\npredicted_label = [190, 50, 165, 44, 171, 5, 79, 108, 98, 83]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LeNet5","metadata":{}},{"cell_type":"code","source":"from keras.layers import BatchNormalization","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Design the model based on the LeNet5 architecture\nmodel = Sequential() \n\n#BatchNormalization layer is used to filter the input image and scaling\n#mean output close to 0 and the output standard deviation close to 1.\n#The input shape is not the usual 28,28,1 provided in the LeNet architecture\n#As the images from this dataset has a lot of details\n#Decreasing the size of the image will not help the model to capture the features \n#in the image\n#So, we choose 120,120,3 as it seems like this is where the model starts to \n#capture features from the images\nmodel.add(BatchNormalization(), input_shape=(120,120,3))\n\n#32 filters were used, as said the image has a lot of details, low number of filters\n#will not help in capturing the details of the image\n#Activation tanh is used because 'relu' had been proving that it has bad relation\n#with 'softmax', 'tanh' provide a more promising result\nmodel.add(Conv2D(32, kernel_size=(5, 5), activation='tanh')) \nmodel.add(MaxPooling2D(pool_size=(2, 2))) \nmodel.add(Conv2D(48, kernel_size=(5, 5), activation='tanh')) \nmodel.add(MaxPooling2D(pool_size=(2, 2))) \nmodel.add(Flatten()) \nmodel.add(Dense(512, activation='tanh')) \nmodel.add(Dropout(0.25))\nmodel.add(Dense(496, activation='tanh')) \nmodel.add(Dropout(0.25))\nmodel.add(Dense(196, activation='softmax')) \n\nmodel.summary()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1.0e-5),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(), #for cases with multiple categories for conversions as shown below\n    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n\n)\n\nhistory = model.fit(train_ds, epochs=50, validation_data=test_ds)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['val_sparse_categorical_accuracy'])\nplt.title('model training accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images = []\nlabels = []\n\nfor i,(image_batch,label) in enumerate(test_ds.take(1)):\n    image_batch = image_batch.numpy()\n    label = label.numpy()\n    image_batch = np.expand_dims(image_batch, axis=0)\n    print(image_batch.shape)\n    images.append(image_batch)\n    labels.append(label)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classes = model.predict(iamges)\nclasses = np.argmax(classes)\nprint(classes)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"true_label = [141, 2, 87, 35, 189, 111, 154, 78, 98, 94]\npredicted_label = []","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(precision_score(true_label, predicted_label, average = 'weighted'))\nprint(f1_score(true_label, predicted_label, average = 'weighted'))\nprint(recall_score(true_label, predicted_label, average = 'weighted'))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GoogleNet","metadata":{}},{"cell_type":"code","source":"base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224,224,3))\n\n#Freeze\nbase_model.trainable = True\n\n#Declare input layer\ninputs = tf.keras.Input(shape=(height, width, 3))\n\n#Normalization Layer\nnorm_layer = keras.layers.experimental.preprocessing.Normalization()\nmean = np.array([127.5] * 3)\nvar = mean ** 2\n# Scale inputs to [-1, +1]\nx = norm_layer(inputs)\nnorm_layer.set_weights([mean, var])\n\n#ResNet50 Architecture\nx = base_model(x, training=False) \nx = keras.layers.GlobalAveragePooling2D()(x) \n\n#Dropout to improve result and reduce overfitting\nx = keras.layers.Dropout(0.25)(x)  \nx = keras.layers.Dense(1024, activation='relu')(x)\noutputs = keras.layers.Dense(num_outputs, activation=\"softmax\")(x) \n\n#Here we don't use Sequential as it provides a minimal improvement\nmodel = keras.Model(inputs, outputs)\n\n#Summary of the model layers\nmodel.summary()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Additional Note\n**From now on, I stop rewriting the same code for training the model.**\n**Here I use the model.compile() from before to do the training**\n**And also the other function that has been repeating**","metadata":{}},{"cell_type":"markdown","source":"# AlexNet","metadata":{}},{"cell_type":"code","source":"model = Sequential() \nmodel.add(Conv2D(96, kernel_size=(11, 11), strides=(4, 4), activation='relu', input_shape=(28, 28, 1))) \nmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2))) \nmodel.add(Conv2D(256, kernel_size=(5, 5), padding='same', activation='relu')) \nmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2))) \nmodel.add(Conv2D(384, kernel_size=(3, 3), padding='same', activation='relu')) \nmodel.add(Conv2D(384, kernel_size=(3, 3), padding='same', activation='relu')) \nmodel.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu')) \nmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2))) \nmodel.add(Flatten()) \nmodel.add(Dense(4096, activation='relu')) \nmodel.add(Dropout(0.5)) \nmodel.add(Dense(4096, activation='relu')) \nmodel.add(Dropout(0.5)) \nmodel.add(Dense(196, activation='softmax')) \n\nmodel.summary()","metadata":{},"outputs":[],"execution_count":null}]}